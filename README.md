<div align="center">

# ğŸ” NovelVerified.AI

**AI-Powered Novel Claim Verification System**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Tests](https://img.shields.io/badge/tests-55%20passed-brightgreen.svg)](#testing)

*Verify character backstory claims against actual novel text using RAG + LLM*

*Supports both **Claude API** and **Local LLM** (Ollama)*

</div>

---

## ğŸ“– Overview

NovelVerified.AI is an intelligent system that determines whether **character backstory claims** about literary works are **supported** or **contradicted** by the actual text of the novels.

### Example

> **Claim:** "Edmond Dantes was imprisoned for fourteen years"  
> **Novel:** *The Count of Monte Cristo*  
> **Verdict:** âœ… **SUPPORTED** (confidence: 0.95)

---

## âœ¨ Features

- ğŸ¤– **7-Agent Pipeline** - Modular architecture from ingestion to results
- ğŸ” **Semantic Search** - FAISS vector index with sentence-transformers
- ğŸ§  **Flexible LLM Backend** - Claude API or **local Ollama** (runs on your GPU!)
- ğŸ“Š **Modern Dashboard** - React + Tailwind UI for exploring results
- ğŸ”„ **Resumable Processing** - Continue from where you left off
- ğŸ“ **Detailed Dossiers** - Human-readable Markdown reports per claim
- âœ… **Comprehensive Tests** - 55+ pytest tests with mocked APIs

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   novels/*.txt       â”‚       train.csv / test.csv              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                              â”‚
           â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ingestion Agent    â”‚       â”‚    Claim Parser      â”‚
â”‚   (chunk novels)     â”‚       â”‚    (parse CSV)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                              â”‚
           â–¼                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚   Embedding Agent    â”‚                  â”‚
â”‚   (FAISS index)      â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
           â”‚                              â”‚
           â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Retriever Agent                            â”‚
â”‚              (find relevant passages per claim)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Reasoning Agent                            â”‚
â”‚                    (Claude API verdicts)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                       â”‚
           â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dossier Writer     â”‚             â”‚   Results Aggregator       â”‚
â”‚   (Markdown reports) â”‚             â”‚   (CSV output)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- **Option A:** [Anthropic API key](https://console.anthropic.com/) (cloud)
- **Option B:** [Ollama](https://ollama.com/) (local, free)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/NovelVerified.AI.git
cd NovelVerified.AI

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Set up environment
cp .env.example .env
# Edit .env and add your ANTHROPIC_API_KEY
```

### Run the Pipeline

```bash
# Full pipeline with Claude API
python run_all.py

# Full pipeline with LOCAL LLM (no API needed!)
python run_all.py --local

# Test mode (limited claims)
python run_all.py --test-mode

# Skip LLM calls (use cached verdicts)
python run_all.py --skip-reasoning

# Start from specific stage
python run_all.py --start-from reasoning
```

### ğŸ  Local LLM Setup (Ollama)

Run entirely on your machine with no API costs:

```bash
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull a model (choose based on your GPU VRAM)
ollama pull phi3:mini          # 4GB VRAM - fast
ollama pull mistral:7b-instruct-q4_0  # 5GB VRAM - better quality
ollama pull llama3.2:3b        # 3GB VRAM - lightweight

# 3. Run the pipeline locally
python run_all.py --local
```

| Model | VRAM Required | Speed |
|-------|---------------|-------|
| phi3:mini | ~4GB | ~2-3 sec/claim |
| mistral:7b-q4 | ~5GB | ~4-5 sec/claim |
| llama3.2:3b | ~3GB | ~2 sec/claim |

### Start the Dashboard

```bash
# Terminal 1: Start Flask API
python flask_api/app.py

# Terminal 2: Start React frontend
cd frontend
npm install
npm run dev
```

Open http://localhost:5173 to view the dashboard.

---

## ğŸ“ Project Structure

```
NovelVerified.AI/
â”œâ”€â”€ agents/                  # Pipeline agents
â”‚   â”œâ”€â”€ ingestion_agent.py   # Chunk novels into segments
â”‚   â”œâ”€â”€ embedding_agent.py   # Create FAISS vector index
â”‚   â”œâ”€â”€ claim_parser.py      # Parse claims from CSV
â”‚   â”œâ”€â”€ retriever_agent.py   # Find relevant passages
â”‚   â”œâ”€â”€ reasoning_agent.py       # Claude API verification
â”‚   â”œâ”€â”€ reasoning_agent_local.py # Local Ollama verification
â”‚   â”œâ”€â”€ dossier_writer.py        # Generate Markdown reports
â”‚   â”œâ”€â”€ results_aggregator.py    # Compile final CSV
â”‚   â””â”€â”€ utils.py                 # Shared utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ novels/              # Source novel .txt files
â”‚   â”œâ”€â”€ train.csv            # Training claims (with labels)
â”‚   â””â”€â”€ test.csv             # Test claims
â”œâ”€â”€ flask_api/
â”‚   â””â”€â”€ app.py               # REST API server
â”œâ”€â”€ frontend/                # React + Vite + Tailwind dashboard
â”œâ”€â”€ tests/                   # Pytest test suite
â”œâ”€â”€ run_all.py               # Pipeline orchestrator
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ .env.example             # Environment template
```

### Generated Directories

| Directory | Contents |
|-----------|----------|
| `chunks/` | Chunked novel text (JSONL) |
| `index/` | FAISS index + metadata |
| `claims/` | Parsed claims (JSONL) |
| `evidence/` | Retrieved passages per claim |
| `verdicts/` | Claude API verdicts (JSON) |
| `dossiers/` | Human-readable reports (Markdown) |
| `output/` | Final results.csv |

---

## ğŸ”§ Configuration

Edit `.env` to configure:

```env
# Option A: Claude API
ANTHROPIC_API_KEY=sk-ant-...
CLAUDE_MODEL=claude-sonnet-4-20250514

# Option B: Local Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=phi3:mini

# Flask server
FLASK_HOST=127.0.0.1
FLASK_PORT=5000
FLASK_DEBUG=true
```

---

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=agents --cov=flask_api

# Run specific test file
python -m pytest tests/test_reasoning_agent.py -v

# Run only unit tests
python -m pytest tests/ -v -m unit
```

Current status: **55 tests passing** âœ…

---

## ğŸ“Š API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/results` | GET | All verification results |
| `/api/dossier/<id>` | GET | Markdown dossier for claim |
| `/api/verdict/<id>` | GET | Raw verdict JSON |
| `/api/evidence/<id>` | GET | Retrieved evidence |
| `/api/stats` | GET | Summary statistics |
| `/api/books` | GET | List of books |
| `/api/characters` | GET | List of characters |
| `/download/results.csv` | GET | Download results file |

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [Anthropic Claude](https://www.anthropic.com/) - Cloud AI reasoning
- [Ollama](https://ollama.com/) - Local LLM runtime
- [Sentence Transformers](https://www.sbert.net/) - Embeddings
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search
- Classic novels from [Project Gutenberg](https://www.gutenberg.org/)

---

<div align="center">

**Built with â¤ï¸ for literary AI research**

</div>
