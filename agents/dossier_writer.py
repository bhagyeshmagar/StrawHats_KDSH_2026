"""
Dossier Writer Agent - Generates human-readable Markdown dossiers.

For each verdict in verdicts/{claim_id}.json, generates a formatted
Markdown dossier in dossiers/{claim_id}.md.
"""

import json
from pathlib import Path
from datetime import datetime

# Configuration
VERDICTS_DIR = Path("verdicts")
EVIDENCE_DIR = Path("evidence")
OUTPUT_DIR = Path("dossiers")

# Verdict badges
BADGES = {
    "supported": "âœ… **SUPPORTED**",
    "contradicted": "âŒ **CONTRADICTED**",
    "undetermined": "âš ï¸ **UNDETERMINED**"
}


def get_confidence_bar(confidence: float) -> str:
    """Generate a visual confidence bar."""
    filled = int(confidence * 10)
    empty = 10 - filled
    return f"[{'â–ˆ' * filled}{'â–‘' * empty}] {confidence:.0%}"


def format_evidence(evidence: list[dict]) -> str:
    """Format evidence passages for the dossier."""
    sections = []
    
    for i, ev in enumerate(evidence, 1):
        text = ev["text"]
        if len(text) > 800:
            text = text[:800] + "..."
            
        sections.append(f"""### Evidence {i}
**Book:** {ev['book']}  
**Chunk:** {ev['chunk_idx']} (chars {ev['char_start']}-{ev['char_end']})  
**Relevance Score:** {ev['score']:.3f}

> {text}
""")
    
    return "\n".join(sections)


def format_spans(spans: list, label: str, emoji: str) -> str:
    """Format supporting/contradicting spans."""
    if not spans:
        return f"*No {label.lower()} spans identified.*"
    
    items = [f'{emoji} "{span}"' for span in spans[:5]]  # Limit to 5
    return "\n".join(items)


def generate_dossier(verdict: dict, evidence_data: dict) -> str:
    """Generate Markdown dossier content."""
    claim_id = verdict["claim_id"]
    badge = BADGES.get(verdict["verdict"], "â“ **UNKNOWN**")
    confidence_bar = get_confidence_bar(verdict.get("confidence", 0))
    
    supporting = format_spans(
        verdict.get("supporting_spans", []),
        "supporting",
        "ðŸ“—"
    )
    contradicting = format_spans(
        verdict.get("contradicting_spans", []),
        "contradicting", 
        "ðŸ“•"
    )
    
    evidence_section = format_evidence(evidence_data.get("evidence", []))
    
    return f"""# Claim Dossier: {claim_id}

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

## Claim Information

| Field | Value |
|-------|-------|
| **Claim ID** | {claim_id} |
| **Character** | {evidence_data.get('character', 'Unknown')} |
| **Book** | {evidence_data.get('book_name', 'Unknown')} |

### Claim Text

> {evidence_data.get('claim_text', 'N/A')}

---

## Verdict

{badge}

**Confidence:** {confidence_bar}

### Reasoning

{verdict.get('reasoning', 'No reasoning provided.')}

---

## Key Spans

### Supporting Evidence
{supporting}

### Contradicting Evidence
{contradicting}

---

## Retrieved Passages

{evidence_section}

---

*This dossier was automatically generated by NovelVerified.AI*
"""


def main():
    """Main entry point for dossier writer agent."""
    print("=" * 60)
    print("DOSSIER WRITER AGENT - Markdown Generation")
    print("=" * 60)
    
    # Check directories
    verdict_files = list(VERDICTS_DIR.glob("*.json"))
    if not verdict_files:
        print(f"ERROR: No verdict files found in {VERDICTS_DIR}/")
        print("  Run reasoning_agent.py first.")
        return
    
    print(f"Found {len(verdict_files)} verdict files")
    
    # Create output directory
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Process each verdict
    for i, verdict_file in enumerate(verdict_files):
        claim_id = verdict_file.stem
        
        # Load verdict
        with open(verdict_file, "r", encoding="utf-8") as f:
            verdict = json.load(f)
        
        # Load evidence data for additional context
        evidence_file = EVIDENCE_DIR / f"{claim_id}.json"
        if evidence_file.exists():
            with open(evidence_file, "r", encoding="utf-8") as f:
                evidence_data = json.load(f)
        else:
            evidence_data = {"evidence": []}
        
        # Generate dossier
        dossier = generate_dossier(verdict, evidence_data)
        
        # Save
        output_file = OUTPUT_DIR / f"{claim_id}.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(dossier)
        
        if (i + 1) % 20 == 0 or i == len(verdict_files) - 1:
            print(f"  Generated {i + 1}/{len(verdict_files)} dossiers")
    
    print("=" * 60)
    print(f"Dossiers saved to {OUTPUT_DIR}/")


if __name__ == "__main__":
    main()
